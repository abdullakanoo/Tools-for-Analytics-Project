{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d01f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f66181",
   "metadata": {},
   "source": [
    "# Part 1:  Data preprocessing\n",
    "\n",
    "\n",
    "This project deals with a lot of data that can be too much for a personal computer to handle effectively. For instance, each month of yellow taxi data is about 2GB in size. Therefore, it’s imperative that you remove any unnecessary and invalid data, use appropriate data types when possible, and take samples.\n",
    "\n",
    "For this part, you will need to use the requests, BeautifulSoup, and pandas packages to help you programmatically download and clean every Yellow Taxi CSV file needed. Cleaning the data includes: removing unnecessary columns and invalid data points, normalizing column names, and removing trips that start and/or end outside of the following latitude/longitude coordinate box: (40.560445, -74.242330) and (40.908524, -73.717047). \n",
    "\n",
    "While you will not need to programmatically download the Uber data, you will need to load it in from your computer, and clean the dataset as you did with the Yellow Taxi datasets.\n",
    "\n",
    "Each month of Yellow Taxi data contains millions of trips. However, the provided Uber dataset is only a sampling of all data. Therefore, you will need to generate a sampling of Yellow Taxi data that’s roughly equal to the sample size of the Uber dataset.\n",
    "\n",
    "Also within this part, define a function that calculates the distance between two coordinates in kilometers that only uses the `math` module from the standard library. Write at least one unit test that tests this calculation function. \n",
    "\n",
    "Using that function that calculates the distance in kilometers between two coordinates, add a column to each dataset that contains the distance between the pickup and dropoff location.\n",
    "\n",
    "Finally, load in the weather datasets from your computer, and clean each dataset, including only the dates & columns needed to answer the questions in the other parts of the project.\n",
    "\n",
    "Tips:\n",
    "Downloading Yellow Taxi data can take a while per file since each file is so large. Consider saving the sample data for each month to your computer in case you need to step away, and load it back in when you return.\n",
    "Relatedly, make use of your .gitignore file to avoid committing the Yellow Taxi sample dataset CSV files to your repo.\n",
    "Read ahead to figure out which columns are absolutely necessary for each dataset.\n",
    "Be mindful of the data types for each column, which will make it easier for yourself when storing and filtering data later on.\n",
    "Use the re module to help pull out the desired links for Yellow Taxi CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7706827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import datetime \n",
    "import io\n",
    "\n",
    "import bs4\n",
    "\n",
    "import math\n",
    "\n",
    "import geopy.distance\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe2c82",
   "metadata": {},
   "source": [
    "# Defining function to calculate the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5b4498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279.35290160430094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[278.45856843965987]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_distance(from_coord, to_coord):\n",
    "    def deg2rad(deg):\n",
    "        return deg * (np.pi/180)\n",
    "    def hav(theta):\n",
    "        return np.sin(theta/2)**2\n",
    "\n",
    "    from_coord = list(from_coord)\n",
    "    to_coord = list(to_coord)\n",
    "    d = []\n",
    "    \n",
    "    for i in range(len(from_coord)):\n",
    "        x1 = from_coord[i][0]\n",
    "        y1 = from_coord[i][1]\n",
    "        x2 = to_coord[i][0]\n",
    "        y2 = to_coord[i][1]\n",
    "        R = 6371.009 # Radius of the earth in km\n",
    "        dLat = deg2rad(x2-x1)\n",
    "        dLon = deg2rad(y2-y1) \n",
    "        c = np.arcsin(np.sqrt(hav(dLat) + (1 - hav(deg2rad(x1 - x2)) - hav(deg2rad(x1 + x2)))*hav(dLon)))\n",
    "        d.append(2 * R * c) #Distance in km\n",
    "    return d\n",
    "\n",
    "def test_calculate_distance():\n",
    "    coords_1 = (52.2296756, 21.0122287)\n",
    "    coords_2 = (52.406374, 16.9251681)\n",
    "\n",
    "    print (geopy.distance.geodesic(coords_1, coords_2).km)\n",
    "\n",
    "test_calculate_distance()\n",
    "from_coord = [[52.2296756, 21.0122287]]\n",
    "to_coord = [[52.406374, 16.9251681]]\n",
    "calculate_distance(from_coord, to_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c9047",
   "metadata": {},
   "source": [
    "# Getting the number of samples of each year and month of the uber rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639102ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples is:  200000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24238194</td>\n",
       "      <td>2015-05-07 19:52:06.0000003</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27835199</td>\n",
       "      <td>2009-07-17 20:04:56.0000002</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44984355</td>\n",
       "      <td>2009-08-24 21:45:00.00000061</td>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25894730</td>\n",
       "      <td>2009-06-26 08:22:21.0000001</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17610152</td>\n",
       "      <td>2014-08-28 17:47:00.000000188</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>42598914</td>\n",
       "      <td>2012-10-28 10:49:00.00000053</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2012-10-28 10:49:00 UTC</td>\n",
       "      <td>-73.987042</td>\n",
       "      <td>40.739367</td>\n",
       "      <td>-73.986525</td>\n",
       "      <td>40.740297</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>16382965</td>\n",
       "      <td>2014-03-14 01:09:00.0000008</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2014-03-14 01:09:00 UTC</td>\n",
       "      <td>-73.984722</td>\n",
       "      <td>40.736837</td>\n",
       "      <td>-74.006672</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>27804658</td>\n",
       "      <td>2009-06-29 00:42:00.00000078</td>\n",
       "      <td>30.9</td>\n",
       "      <td>2009-06-29 00:42:00 UTC</td>\n",
       "      <td>-73.986017</td>\n",
       "      <td>40.756487</td>\n",
       "      <td>-73.858957</td>\n",
       "      <td>40.692588</td>\n",
       "      <td>2</td>\n",
       "      <td>2009</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>20259894</td>\n",
       "      <td>2015-05-20 14:56:25.0000004</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2015-05-20 14:56:25 UTC</td>\n",
       "      <td>-73.997124</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>11951496</td>\n",
       "      <td>2010-05-15 04:08:00.00000076</td>\n",
       "      <td>14.1</td>\n",
       "      <td>2010-05-15 04:08:00 UTC</td>\n",
       "      <td>-73.984395</td>\n",
       "      <td>40.720077</td>\n",
       "      <td>-73.985508</td>\n",
       "      <td>40.768793</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                            key  fare_amount  \\\n",
       "0         24238194    2015-05-07 19:52:06.0000003          7.5   \n",
       "1         27835199    2009-07-17 20:04:56.0000002          7.7   \n",
       "2         44984355   2009-08-24 21:45:00.00000061         12.9   \n",
       "3         25894730    2009-06-26 08:22:21.0000001          5.3   \n",
       "4         17610152  2014-08-28 17:47:00.000000188         16.0   \n",
       "...            ...                            ...          ...   \n",
       "199995    42598914   2012-10-28 10:49:00.00000053          3.0   \n",
       "199996    16382965    2014-03-14 01:09:00.0000008          7.5   \n",
       "199997    27804658   2009-06-29 00:42:00.00000078         30.9   \n",
       "199998    20259894    2015-05-20 14:56:25.0000004         14.5   \n",
       "199999    11951496   2010-05-15 04:08:00.00000076         14.1   \n",
       "\n",
       "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0       2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1       2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "2       2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
       "3       2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
       "4       2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
       "...                         ...               ...              ...   \n",
       "199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n",
       "199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n",
       "199997  2009-06-29 00:42:00 UTC        -73.986017        40.756487   \n",
       "199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n",
       "199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  passenger_count  Year Month  \n",
       "0              -73.999512         40.723217                1  2015    05  \n",
       "1              -73.994710         40.750325                1  2009    07  \n",
       "2              -73.962565         40.772647                1  2009    08  \n",
       "3              -73.965316         40.803349                3  2009    06  \n",
       "4              -73.973082         40.761247                5  2014    08  \n",
       "...                   ...               ...              ...   ...   ...  \n",
       "199995         -73.986525         40.740297                1  2012    10  \n",
       "199996         -74.006672         40.739620                1  2014    03  \n",
       "199997         -73.858957         40.692588                2  2009    06  \n",
       "199998         -73.983215         40.695415                1  2015    05  \n",
       "199999         -73.985508         40.768793                1  2010    05  \n",
       "\n",
       "[200000 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Getting the number of samples of each year and month of the uber rides.\n",
    "file = \"uber_rides_sample.csv\"\n",
    "df_uber = pd.read_csv(file)\n",
    "\n",
    "\n",
    "initial_year = 2009\n",
    "ending_year = 2015\n",
    "\n",
    "initial_month = 1\n",
    "ending_month = 12\n",
    "\n",
    "counter_sample = {}\n",
    "\n",
    "df_uber['Year'] = df_uber['key'].str.slice(0, 4)\n",
    "df_uber['Month'] = df_uber['key'].str.slice(5, 7)\n",
    "\n",
    "for year in range(initial_year,ending_year + 1):\n",
    "    for month in range(initial_month, ending_month + 1):\n",
    "        if month < 10:\n",
    "            aux = df_uber[df_uber[\"Year\"] == str(year)]\n",
    "            aux = aux[aux[\"Month\"] == \"0\"+str(month)]\n",
    "            counter_sample[str(year)+\"-0\"+str(month)] = len(aux)\n",
    "        else: \n",
    "            aux = df_uber[df_uber[\"Year\"] == str(year)]\n",
    "            aux = aux[aux[\"Month\"] == str(month)]\n",
    "            counter_sample[str(year)+\"-\"+str(month)] = len(aux)\n",
    "            \n",
    "counter_sample  \n",
    "values = sum(counter_sample.values())\n",
    "print(\"The total number of samples is: \",values)\n",
    "#print(counter_sample)\n",
    "df_uber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7915be",
   "metadata": {},
   "source": [
    "# Function to find the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43da1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-12.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-12.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-12.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-12.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-12.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-12.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-01.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-02.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-03.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-04.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-05.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-06.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-07.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-08.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-09.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-10.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-11.csv',\n",
       " 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-12.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Function to find the urls\n",
    "def find_taxi_csv_urls():\n",
    "    years = [2009,2010,2011,2012,2013,2014,2015]\n",
    "    TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    content = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(content.text, 'html.parser')\n",
    "    divs = soup.find_all(\"div\")\n",
    "    new_divs1 = []\n",
    "    new_divs2 = []\n",
    "    \n",
    "    ancors = soup.find_all(\"a\")\n",
    "    ancors_yellow = []\n",
    "    \n",
    "    for i in ancors:\n",
    "        if 'title' in i.attrs.keys() and i[\"title\"] == \"Yellow Taxi Trip Records\" and int(i[\"href\"][-11:-7]) in years:\n",
    "            ancors_yellow.append(i[\"href\"])\n",
    "    \n",
    "    return ancors_yellow\n",
    "\n",
    "\n",
    "\n",
    "urls = find_taxi_csv_urls()\n",
    "urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379870d4",
   "metadata": {},
   "source": [
    "#  Sampling the yellow taxi rides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d258fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-12.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2014-12.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-12.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2012-12.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2011-12.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-12.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-01.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-02.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-03.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-04.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-05.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-06.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-07.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-08.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-09.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-10.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-11.csv', 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-12.csv']\n"
     ]
    }
   ],
   "source": [
    "#Sampling the yellow taxi rides.\n",
    "\n",
    "\n",
    "def sampling(urls):\n",
    "    for url in urls:\n",
    "        year = url[-11:-7]\n",
    "        month = url[-6:-4]\n",
    "        print(url)\n",
    "        df = pd.read_csv(url, on_bad_lines='skip')\n",
    "        print(\"The length of the original file is: \", len(df))\n",
    "\n",
    "        df_taxi = df.sample(n = counter_sample[url[-11:-4]], random_state = 1)\n",
    "        print(\"The length of the sample (year, month) = ({},{}) is {}\".format(year,month,len(df_taxi)))   \n",
    "\n",
    "        df_taxi.to_csv(\"taxi_rides_sample\"+ year +\"-\"+month +\".csv\")\n",
    "        print(\"ok\")\n",
    "\n",
    "        \n",
    "urls = find_taxi_csv_urls()#[-23:-15]\n",
    "print(urls)\n",
    "#df = pd.read_csv(urls)\n",
    "#Falta el 37. Falta 2010        \n",
    "#sampling(urls)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6a1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9e8140",
   "metadata": {},
   "source": [
    "# Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e12c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(year, month) = (2009, 01)\n",
      "(year, month) = (2009, 02)\n",
      "(year, month) = (2009, 03)\n",
      "(year, month) = (2009, 04)\n",
      "(year, month) = (2009, 05)\n",
      "(year, month) = (2009, 06)\n",
      "(year, month) = (2009, 07)\n",
      "(year, month) = (2009, 08)\n",
      "(year, month) = (2009, 09)\n",
      "(year, month) = (2009, 10)\n",
      "(year, month) = (2009, 11)\n",
      "(year, month) = (2009, 12)\n",
      "(year, month) = (2010, 01)\n",
      "(year, month) = (2010, 02)\n",
      "(year, month) = (2010, 03)\n",
      "(year, month) = (2010, 04)\n",
      "(year, month) = (2010, 05)\n",
      "(year, month) = (2010, 06)\n",
      "(year, month) = (2010, 07)\n",
      "(year, month) = (2010, 08)\n",
      "(year, month) = (2010, 09)\n",
      "(year, month) = (2010, 10)\n",
      "(year, month) = (2010, 11)\n",
      "(year, month) = (2010, 12)\n",
      "(year, month) = (2011, 01)\n",
      "(year, month) = (2011, 02)\n",
      "(year, month) = (2011, 03)\n",
      "(year, month) = (2011, 04)\n",
      "(year, month) = (2011, 05)\n",
      "(year, month) = (2011, 06)\n",
      "(year, month) = (2011, 07)\n",
      "(year, month) = (2011, 08)\n",
      "(year, month) = (2011, 09)\n",
      "(year, month) = (2011, 10)\n",
      "(year, month) = (2011, 11)\n",
      "(year, month) = (2011, 12)\n",
      "(year, month) = (2012, 01)\n",
      "(year, month) = (2012, 02)\n",
      "(year, month) = (2012, 03)\n",
      "(year, month) = (2012, 04)\n",
      "(year, month) = (2012, 05)\n",
      "(year, month) = (2012, 06)\n",
      "(year, month) = (2012, 07)\n",
      "(year, month) = (2012, 08)\n",
      "(year, month) = (2012, 09)\n",
      "(year, month) = (2012, 10)\n",
      "(year, month) = (2012, 11)\n",
      "(year, month) = (2012, 12)\n",
      "(year, month) = (2013, 01)\n",
      "(year, month) = (2013, 02)\n",
      "(year, month) = (2013, 03)\n",
      "(year, month) = (2013, 04)\n",
      "(year, month) = (2013, 05)\n",
      "(year, month) = (2013, 06)\n",
      "(year, month) = (2013, 07)\n",
      "(year, month) = (2013, 08)\n",
      "(year, month) = (2013, 09)\n",
      "(year, month) = (2013, 10)\n",
      "(year, month) = (2013, 11)\n",
      "(year, month) = (2013, 12)\n",
      "(year, month) = (2014, 01)\n",
      "(year, month) = (2014, 02)\n",
      "(year, month) = (2014, 03)\n",
      "(year, month) = (2014, 04)\n",
      "(year, month) = (2014, 05)\n",
      "(year, month) = (2014, 06)\n",
      "(year, month) = (2014, 07)\n",
      "(year, month) = (2014, 08)\n",
      "(year, month) = (2014, 09)\n",
      "(year, month) = (2014, 10)\n",
      "(year, month) = (2014, 11)\n",
      "(year, month) = (2014, 12)\n",
      "(year, month) = (2015, 01)\n",
      "(year, month) = (2015, 02)\n",
      "(year, month) = (2015, 03)\n",
      "(year, month) = (2015, 04)\n",
      "(year, month) = (2015, 05)\n",
      "(year, month) = (2015, 06)\n",
      "(year, month) = (2015, 07)\n",
      "(year, month) = (2015, 08)\n",
      "(year, month) = (2015, 09)\n",
      "(year, month) = (2015, 10)\n",
      "(year, month) = (2015, 11)\n",
      "(year, month) = (2015, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>Trip_Pickup_DateTime</th>\n",
       "      <th>Trip_Dropoff_DateTime</th>\n",
       "      <th>Passenger_Count</th>\n",
       "      <th>Trip_Distance</th>\n",
       "      <th>Start_Lon</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Rate_Code</th>\n",
       "      <th>store_and_forward</th>\n",
       "      <th>...</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>extra</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>RatecodeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7561035</td>\n",
       "      <td>VTS</td>\n",
       "      <td>2009-01-18 03:05:00</td>\n",
       "      <td>2009-01-18 03:05:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-73.988533</td>\n",
       "      <td>40.737097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12786419</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2009-01-11 16:16:20</td>\n",
       "      <td>2009-01-11 16:24:44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-74.007916</td>\n",
       "      <td>40.725825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1633484</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2009-01-27 22:31:35</td>\n",
       "      <td>2009-01-27 22:32:49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-73.957135</td>\n",
       "      <td>40.770662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10767030</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2009-01-10 14:20:57</td>\n",
       "      <td>2009-01-10 14:31:58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>-73.981873</td>\n",
       "      <td>40.748760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11727611</td>\n",
       "      <td>VTS</td>\n",
       "      <td>2009-01-18 00:48:00</td>\n",
       "      <td>2009-01-18 00:56:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>-73.993790</td>\n",
       "      <td>40.741527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>587414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015-06-01 12:19:07</td>\n",
       "      <td>2015-06-01 13:07:06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>4963741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015-06-17 08:56:36</td>\n",
       "      <td>2015-06-17 09:01:17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>45385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-06-04 15:52:47</td>\n",
       "      <td>2015-06-04 15:58:48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>5879609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015-06-19 10:04:12</td>\n",
       "      <td>2015-06-19 10:22:39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>5935913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-06-17 21:00:57</td>\n",
       "      <td>2015-06-17 21:18:54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 vendor_name Trip_Pickup_DateTime Trip_Dropoff_DateTime  \\\n",
       "0       7561035         VTS  2009-01-18 03:05:00   2009-01-18 03:05:00   \n",
       "1      12786419         CMT  2009-01-11 16:16:20   2009-01-11 16:24:44   \n",
       "2       1633484         CMT  2009-01-27 22:31:35   2009-01-27 22:32:49   \n",
       "3      10767030         CMT  2009-01-10 14:20:57   2009-01-10 14:31:58   \n",
       "4      11727611         VTS  2009-01-18 00:48:00   2009-01-18 00:56:00   \n",
       "...         ...         ...                  ...                   ...   \n",
       "2156     587414         NaN                  NaN                   NaN   \n",
       "2157    4963741         NaN                  NaN                   NaN   \n",
       "2158      45385         NaN                  NaN                   NaN   \n",
       "2159    5879609         NaN                  NaN                   NaN   \n",
       "2160    5935913         NaN                  NaN                   NaN   \n",
       "\n",
       "      Passenger_Count  Trip_Distance  Start_Lon  Start_Lat  Rate_Code  \\\n",
       "0                 1.0           1.14 -73.988533  40.737097        NaN   \n",
       "1                 1.0           1.90 -74.007916  40.725825        NaN   \n",
       "2                 1.0           0.50 -73.957135  40.770662        NaN   \n",
       "3                 2.0           2.20 -73.981873  40.748760        NaN   \n",
       "4                 2.0           1.48 -73.993790  40.741527        NaN   \n",
       "...               ...            ...        ...        ...        ...   \n",
       "2156              NaN            NaN        NaN        NaN        NaN   \n",
       "2157              NaN            NaN        NaN        NaN        NaN   \n",
       "2158              NaN            NaN        NaN        NaN        NaN   \n",
       "2159              NaN            NaN        NaN        NaN        NaN   \n",
       "2160              NaN            NaN        NaN        NaN        NaN   \n",
       "\n",
       "      store_and_forward  ...   tip_amount   tolls_amount  total_amount  \\\n",
       "0                   NaN  ...          NaN            NaN           NaN   \n",
       "1                   NaN  ...          NaN            NaN           NaN   \n",
       "2                   NaN  ...          NaN            NaN           NaN   \n",
       "3                   NaN  ...          NaN            NaN           NaN   \n",
       "4                   NaN  ...          NaN            NaN           NaN   \n",
       "...                 ...  ...          ...            ...           ...   \n",
       "2156                NaN  ...          NaN            NaN           NaN   \n",
       "2157                NaN  ...          NaN            NaN           NaN   \n",
       "2158                NaN  ...          NaN            NaN           NaN   \n",
       "2159                NaN  ...          NaN            NaN           NaN   \n",
       "2160                NaN  ...          NaN            NaN           NaN   \n",
       "\n",
       "      VendorID  tpep_pickup_datetime  tpep_dropoff_datetime  RateCodeID  \\\n",
       "0          NaN                   NaN                    NaN         NaN   \n",
       "1          NaN                   NaN                    NaN         NaN   \n",
       "2          NaN                   NaN                    NaN         NaN   \n",
       "3          NaN                   NaN                    NaN         NaN   \n",
       "4          NaN                   NaN                    NaN         NaN   \n",
       "...        ...                   ...                    ...         ...   \n",
       "2156       2.0   2015-06-01 12:19:07    2015-06-01 13:07:06         1.0   \n",
       "2157       2.0   2015-06-17 08:56:36    2015-06-17 09:01:17         1.0   \n",
       "2158       1.0   2015-06-04 15:52:47    2015-06-04 15:58:48         1.0   \n",
       "2159       2.0   2015-06-19 10:04:12    2015-06-19 10:22:39         1.0   \n",
       "2160       1.0   2015-06-17 21:00:57    2015-06-17 21:18:54         1.0   \n",
       "\n",
       "      extra  improvement_surcharge RatecodeID  \n",
       "0       NaN                    NaN        NaN  \n",
       "1       NaN                    NaN        NaN  \n",
       "2       NaN                    NaN        NaN  \n",
       "3       NaN                    NaN        NaN  \n",
       "4       NaN                    NaN        NaN  \n",
       "...     ...                    ...        ...  \n",
       "2156    0.0                    0.3        NaN  \n",
       "2157    0.0                    0.3        NaN  \n",
       "2158    1.0                    0.3        NaN  \n",
       "2159    0.0                    0.3        NaN  \n",
       "2160    0.5                    0.3        NaN  \n",
       "\n",
       "[200000 rows x 59 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Read csv\n",
    "\n",
    "def read_csv_files(st):\n",
    "    \n",
    "    initial_year = \"2009\"\n",
    "    initial_month = \"01\"\n",
    "    \n",
    "    ending_year = \"2015\"\n",
    "    ending_month = \"12\"\n",
    "    \n",
    "    for year in range(int(initial_year), int(ending_year) + 1):\n",
    "        for month in range(int(initial_month), int(ending_month) + 1):\n",
    "            \n",
    "            if int(month) < 10:\n",
    "                month_s = \"0\" + str(month)\n",
    "            else:\n",
    "                month_s = str(month)\n",
    "            year_s = str(year)\n",
    "            \n",
    "            if year == int(initial_year) and month == int(initial_month):\n",
    "                file = \"taxi_rides_sample\"+ year_s +\"-\"+ month_s +\".csv\"\n",
    "                path = Path(file)\n",
    "                if path.is_file():\n",
    "                    df = pd.read_csv(file)\n",
    "                    if st:\n",
    "                        print(\"(year, month) = ({}, {})\".format(year_s,month_s))\n",
    "            else:\n",
    "                file = \"taxi_rides_sample\"+ year_s +\"-\" + month_s +\".csv\"\n",
    "                path = Path(file)\n",
    "                if path.is_file():\n",
    "                    aux = pd.read_csv(file)\n",
    "                    df = pd.concat([df, aux])\n",
    "                    if st:\n",
    "                        print(\"(year, month) = ({}, {})\".format(year_s,month_s))\n",
    "            \n",
    "    return df\n",
    "\n",
    "df = read_csv_files(True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55407ab",
   "metadata": {},
   "source": [
    "# Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Renaming the columns\n",
    "\n",
    "def df_rename(option, ref, df):\n",
    "    for i in range(len(option)):\n",
    "        df[ref[i]] = df[ref[i]].fillna(df[option[i]])\n",
    "    #df = df[ref]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = read_csv_files(False)\n",
    "option = [\"tolls_amount\", \"total_amount\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\",  \"passenger_count\", \"pickup_latitude\", \"pickup_longitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "ref = [\"Tolls_Amt\", \"Total_Amt\", \"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Tip_Amt\", \"Passenger_Count\", \"Start_Lat\", \"Start_Lon\", \"End_Lon\",\"End_Lat\"]\n",
    "df = df_rename(option, ref, df)\n",
    "\n",
    "option = [\"tolls_amount\", \"total_amount\", \"pickup_datetime\", \"dropoff_datetime\", \"tip_amount\",  \"passenger_count\", \"pickup_latitude\", \"pickup_longitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "ref = [\"Tolls_Amt\", \"Total_Amt\", \"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Tip_Amt\", \"Passenger_Count\", \"Start_Lat\", \"Start_Lon\", \"End_Lon\",\"End_Lat\"]\n",
    "df = df_rename(option, ref, df)\n",
    "df = df[ref]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff1cdf",
   "metadata": {},
   "source": [
    "# Calculating distance and adding it as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating distance and adding it as a column\n",
    "def limits_coord(df):\n",
    "    lower_limit_x = 40.560445\n",
    "    upper_limit_x = 40.908524\n",
    "    lower_limit_y = -74.242330\n",
    "    upper_limit_y = -73.717047\n",
    "    df = df[df[\"Start_Lat\"] > lower_limit_x]\n",
    "    df = df[df[\"Start_Lat\"] < upper_limit_x]\n",
    "    df = df[df[\"End_Lat\"] > lower_limit_x]\n",
    "    df = df[df[\"End_Lat\"] < upper_limit_x]    \n",
    "    \n",
    "    df = df[df[\"Start_Lon\"] > lower_limit_y]\n",
    "    df = df[df[\"Start_Lon\"] < upper_limit_y]\n",
    "    df = df[df[\"End_Lon\"] > lower_limit_y]\n",
    "    df = df[df[\"End_Lon\"] < upper_limit_y]      \n",
    "    \n",
    "    return df\n",
    "\n",
    "def dist(df):\n",
    "    from_coord, to_coord = df[[\"Start_Lat\",\"Start_Lon\"]].values, df[[\"End_Lat\",\"End_Lon\"]].values\n",
    "    distance = calculate_distance(from_coord, to_coord)\n",
    "    df[\"Distance\"] = distance\n",
    "    return df\n",
    "\n",
    "df = limits_coord(df)\n",
    "df = dist(df)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc715549",
   "metadata": {},
   "source": [
    "# Renaming Uber data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming Uber data\n",
    "\n",
    "def df_rename_uber(option, ref, df):\n",
    "    for i in range(len(option)):\n",
    "        print(\"(option, ref): ({},{})\".format(option[i],ref[i]))\n",
    "        df = df.rename(columns = {option[i]: ref[i]})\n",
    "    return df\n",
    "\n",
    "file = \"uber_rides_sample.csv\"\n",
    "df_uber = pd.read_csv(file)\n",
    "\n",
    "option = [\"fare_amount\", \"pickup_datetime\", \"passenger_count\", \"pickup_latitude\", \"pickup_longitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "ref = [\"Total_Amt\", \"Trip_Pickup_DateTime\", \"Passenger_Count\", \"Start_Lat\", \"Start_Lon\", \"End_Lon\",\"End_Lat\"]\n",
    "\n",
    "df_uber = df_rename_uber(option, ref, df_uber)\n",
    "\n",
    "df_uber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f4f6b",
   "metadata": {},
   "source": [
    "# Setting limits to the coordinates of Uber data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50311a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uber = limits_coord(df_uber)\n",
    "df_uber = dist(df_uber)\n",
    "df_uber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b37d1d",
   "metadata": {},
   "source": [
    "# Processing Weather Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    lower_limit_x = 40.560445\n",
    "    upper_limit_x = 40.908524\n",
    "    lower_limit_y = -74.242330\n",
    "    upper_limit_y = -73.717047\n",
    "    df = df[df[\"LATITUDE\"] > lower_limit_x]\n",
    "    df = df[df[\"LATITUDE\"] < upper_limit_x]\n",
    "    df = df[df[\"LONGITUDE\"] > lower_limit_y]\n",
    "    df = df[df[\"LONGITUDE\"] < upper_limit_y]    \n",
    "    df = df[[\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]]\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "    \n",
    "    \n",
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    lower_limit_x = 40.560445\n",
    "    upper_limit_x = 40.908524\n",
    "    lower_limit_y = -74.242330\n",
    "    upper_limit_y = -73.717047\n",
    "    df = df[df[\"LATITUDE\"] > lower_limit_x]\n",
    "    df = df[df[\"LATITUDE\"] < upper_limit_x]\n",
    "    df = df[df[\"LONGITUDE\"] > lower_limit_y]\n",
    "    df = df[df[\"LONGITUDE\"] < upper_limit_y]    \n",
    "    df = df[[\"DATE\", \"DailyAverageWindSpeed\", \"DailyPrecipitation\"]]\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = []\n",
    "    for i in range(2009,2016):\n",
    "        weather_csv_files.append(str(i)+\"_weather.csv\")\n",
    "        \n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    return hourly_data, daily_data\n",
    "\n",
    "#clean_month_weather_data_daily(\"2009_weather.csv\")\n",
    "hourly_data, daily_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74027e71",
   "metadata": {},
   "source": [
    "# Part 2: Storing Data\n",
    "Using SQLAlchemy, create a SQLite database with which you’ll load in your preprocessed datasets.\n",
    "\n",
    "Create and populate four tables: one for your sampled datasets of Yellow Taxi trips, one for Uber trips, one for hourly weather information, and one for daily weather information. Use appropriate data types for each column. \n",
    "\n",
    "Create a schema.sql file that defines each table’s schema. You can use SQLAlchemy within the notebook to help generate this file, (added 2022-04-21) or another programmatic approach, or create this schema file by hand.\n",
    "\n",
    "Tips (added 2022-04-21):\n",
    "The first 48 lines of this gist is a good example of what makes up a schema file.\n",
    "I should be able to run this schema file to create the tables in a database via the SQLite CLI tool. That is, I should be able to run the following command in a Jupyter notebook cell to create a database with the four required tables (it is not expected that you do this yourself for the project, but this is a good sanity check for it to succeed without error):\n",
    "\n",
    "\t!sqlite3 project.db < schema.sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d870e7",
   "metadata": {},
   "source": [
    "# Part 3: Understanding Data\n",
    "For this part, define a SQL query for each of the following questions - one query per question. Save each query as a .sql file, naming it something illustrative of what the query is for, e.g. top_10_hottest_days.sql.\n",
    "\n",
    "For 01-2009 through 06-2015, what hour of the day was the most popular to take a Yellow Taxi? The result should have 24 bins.\n",
    "For the same time frame, what day of the week was the most popular to take an Uber? The result should have 7 bins.\n",
    "What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "Which 10 days in 2014 were the windiest on average, and how many hired trips were made on those days?\n",
    "During Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed? There should be an entry for every single hour, even if no rides were taken, no precipitation was measured, or there was no wind.\n",
    "\n",
    "For each query, be sure to execute it in the notebook so we can see your answers to the question.\n",
    "\n",
    "Tips:\n",
    "You may wish to use SQLAlchemy within the notebook to help craft these queries and query files. You can also use pandas to help check the validity of your queries.\n",
    "You may want to familiarize yourself with COALESCE, WITH, and WITH RECURSIVE expressions for help in answering some of the questions.\n",
    "See appendix of lecture notes from module #10 for more tips/hints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bbed7",
   "metadata": {},
   "source": [
    "# Part 4: Visualizing Data\n",
    "For this final part, you will be creating a bunch of visualizations embedded in your notebook using matplotlib and/or other visualization libraries of your choice. Be sure to define a function for each visualization, then call these functions in separate cells to render each visualization.\n",
    "\n",
    "This is where you can get creative with the look and feel of each visual. All that is required is that each visualization is immediately understandable without necessarily needing to read its associated function (i.e. labeled axes, titles, appropriate plot/graph/visual type, etc). You’re welcome to use Markdown cells to introduce and/or explain each visualization. \n",
    "\n",
    "You can use pandas to help parse data before generating a visualization, but you must read the data from your SQLite database (so, do not read directly from CSV files). CLARIFICATION (2022-04-26): you are able to use pandas dataframes to help with your visualization. But you should be creating those dataframes from querying the SQL tables you need (not by reading from a CSV file).\n",
    "\n",
    "Create an appropriate visualization for the first query/question in part 3.\n",
    "Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month) for both taxis and Ubers combined. Include the 90% confidence interval around the mean in the visualization.\n",
    "Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "Create a scatter plot that compares tip amount versus distance for Yellow Taxi rides. You may remove any outliers how you see fit.\n",
    "Create another scatter plot that compares tip amount versus precipitation amount for Yellow Taxi rides. You may remove any outliers how you see fit.\n",
    "Come up with 3 questions on your own that can be answered based on the data in the 4 tables. Create at least one visualization to answer each question. At least one visualization should require data from at least 3 tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb021246",
   "metadata": {},
   "source": [
    "# We get the df from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c79f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938ade1d",
   "metadata": {},
   "source": [
    "# For 01-2009 through 06-2015, what hour of the day was the most popular to take a Yellow Taxi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee939ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df[\"Trip_Pickup_DateTime\"].values\n",
    "samples = []\n",
    "\n",
    "for i in range(len(dates)):\n",
    "    #print(dates[i])\n",
    "    if type(dates[i]) == str:\n",
    "        aux = int(dates[i][-8:-6])\n",
    "        samples.append(aux)\n",
    "samples\n",
    "\n",
    "plt.hist(samples, density = False, bins = 24)\n",
    "plt.title(\"Histogram Taxi\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c3f22",
   "metadata": {},
   "source": [
    "#  Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month) for both taxis and Ubers combined. Include the 90% confidence interval around the mean in the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac895e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_distance(df):\n",
    "    time = df[\"Trip_Pickup_DateTime\"].values\n",
    "    distance = df[\"Distance\"].values\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501d03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc8a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133d2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11630f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9b439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c9743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee84bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53be46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1553157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcdd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec6a0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e304d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edcd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84c7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fad61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782132dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26414b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
